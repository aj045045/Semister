{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc88aa6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data extraction and cleaning are fundamental steps in any data analysis or machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a56b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data extraction refers to loading data from various sources such as CSV files, databases, APIs, or web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc516999",
   "metadata": {},
   "source": [
    "# Data cleaning is the process of fixing or removing incorrect, corrupted, or irrelevant data within a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdae13",
   "metadata": {},
   "source": [
    "# Handling Missing Values: Filling missing values or removing rows/columns with missing data.\n",
    "# Handling Duplicates: Identifying and removing duplicate records.\n",
    "# Removing Outliers: Identifying extreme values that may distort analysis.\n",
    "# Type Conversions: Ensuring correct data types (e.g., converting strings to dates or numbers).\n",
    "# Standardization: Ensuring consistent data formats, units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5278033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Data Extraction (Reading the CSV file into a DataFrame)\n",
    "# Replace 'data.csv' with the path to your CSV file\n",
    "df = pd.read_csv('any csv file')\n",
    "\n",
    "# Step 2: Exploring the Data\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())  # Display the first 5 rows\n",
    "\n",
    "print(\"\\nSummary of dataset:\")\n",
    "print(df.info())  # Summary of the dataset, including types and missing values\n",
    "\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())  # Check the number of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Cleaning\n",
    "\n",
    "## 3.1 Handling Missing Values\n",
    "# Fill missing 'Age' with the mean value of the Age column\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "\n",
    "# Fill missing 'Salary' with the median salary\n",
    "df['Salary'].fillna(df['Salary'].median(), inplace=True)\n",
    "\n",
    "# Drop rows where 'Name' or 'Department' is missing\n",
    "df.dropna(subset=['Name', 'Department'], inplace=True)\n",
    "\n",
    "## 3.2 Handling Duplicate Rows\n",
    "# Checking for duplicates based on all columns\n",
    "print(\"\\nChecking for duplicate rows:\")\n",
    "print(df.duplicated().sum())  # Number of duplicate rows\n",
    "\n",
    "# Remove duplicate rows if any\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "## 3.3 Handling Outliers (Assume salary greater than 1 million is an outlier)\n",
    "# For example, let's assume salaries greater than 100000 are outliers\n",
    "df = df[df['Salary'] <= 100000]\n",
    "\n",
    "## 3.4 Correct Data Types\n",
    "# Convert 'Joining_Date' to datetime format\n",
    "df['Joining_Date'] = pd.to_datetime(df['Joining_Date'], errors='coerce')\n",
    "\n",
    "# Checking if the conversion worked\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "## 3.5 Standardizing Text Data\n",
    "# Convert all 'Department' names to lowercase for consistency\n",
    "df['Department'] = df['Department'].str.lower()\n",
    "\n",
    "# Step 4: Save the cleaned data to a new CSV\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40cbd1",
   "metadata": {},
   "source": [
    "# Data transformation and normalization are important steps in preparing a dataset for machine learning models. These steps help to bring features into similar ranges, improve model performance, and handle different types of data more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d21452",
   "metadata": {},
   "source": [
    "# Data transformation involves changing the structure or values of the dataset to make it more suitable for analysis. Common transformations include:\n",
    "\n",
    "# Logarithmic transformations: Reduce skewness in highly skewed data( more symmetric and normal).\n",
    "# Square root or power transformations: Normalize variance and reduce skewness.\n",
    "# Encoding categorical variables: Convert categorical variables into numerical form (e.g., one-hot encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed7bb4",
   "metadata": {},
   "source": [
    "# Data Normalization is the process of scaling numeric data to a common range, typically between 0 and 1 or -1 and 1. This is especially important for models that are sensitive to the magnitude of the features, such as:\n",
    "\n",
    "# Min-Max Scaling: Scales data to a range of 0 to 1.\n",
    "# Z-Score (Standardization): Transforms data to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f19af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c569e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Data Extraction (Reading the CSV file into a DataFrame)\n",
    "df = pd.read_csv('any csv file')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Data Transformation\n",
    "\n",
    "## 2.1 Logarithmic Transformation on Skewed Numeric Data\n",
    "# Applying a log transformation on a skewed numeric column 'Salary'\n",
    "df['Log_Salary'] = np.log(df['Salary'] + 1)  # Adding 1 to avoid log(0)\n",
    "\n",
    "## 2.2 Square Root Transformation on Age\n",
    "# Applying square root transformation to reduce the impact of larger values in 'Age'\n",
    "df['Sqrt_Age'] = np.sqrt(df['Age'])\n",
    "\n",
    "## 2.3 Encoding Categorical Variables\n",
    "# One-hot encoding on the 'Department' column\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')# sparse= most element are zero  # Drop first to avoid dummy variable trap\n",
    "encoded_department = encoder.fit_transform(df[['Department']])\n",
    "encoded_department_df = pd.DataFrame(encoded_department, columns=encoder.get_feature_names_out(['Department']))\n",
    "df = pd.concat([df, encoded_department_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Normalization\n",
    "\n",
    "## 3.1 Min-Max Scaling (Normalize Salary and Age to a range of 0 to 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the numeric columns for scaling\n",
    "columns_to_scale = ['Salary', 'Age']\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "## 3.2 Z-Score Standardization (Standardize Salary and Age to have mean=0, std=1)\n",
    "#This method is useful when you want to standardize different features so that\n",
    "#they have the same scale, which is important for certain algorithms like SVM and k-NN.\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Applying standardization on the same columns (Salary and Age)\n",
    "df[['Salary_Std', 'Age_Std']] = standard_scaler.fit_transform(df[['Salary', 'Age']])\n",
    "\n",
    "# Display the transformed dataset\n",
    "print(\"\\nTransformed and Normalized Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 4: Save the transformed data to a new CSV file\n",
    "df.to_csv('transformed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90e073",
   "metadata": {},
   "source": [
    "# Data exploration is the process of getting a sense of the dataset before further analysis. Some key steps include:\n",
    "\n",
    "# Summary Statistics: Descriptive statistics such as mean, median, and standard deviation.\n",
    "# Distribution of Data: Check how data is distributed (normal, skewed, etc.).\n",
    "# Correlation: Look for relationships between different features.\n",
    "# Missing Values: Check for missing or null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fe2f7",
   "metadata": {},
   "source": [
    "# Data visualization helps to present data in graphical formats that make patterns easier to identify. Common techniques include:\n",
    "\n",
    "# Histograms: Show the distribution of data.\n",
    "# Boxplots: Visualize the spread of the data and identify outliers.\n",
    "# Scatter Plots: Show relationships between two numeric variables.\n",
    "# Bar Plots: Compare categorical data.\n",
    "# Heatmaps: Visualize correlation between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Data Extraction (Reading the CSV file into a DataFrame)\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2516b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Exploration\n",
    "\n",
    "## 2.1 Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "## 2.2 Get summary statistics\n",
    "print(\"\\nSummary statistics of numerical columns:\")\n",
    "print(df.describe())\n",
    "\n",
    "## 2.3 Checking for missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "## 2.4 Checking the correlation between numeric variables\n",
    "print(\"\\nCorrelation between numerical variables:\")\n",
    "print(df.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Visualization\n",
    "\n",
    "## 3.1 Distribution of Age and Salary (Histogram)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Age Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['Age'].dropna(), bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Salary Distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['Salary'].dropna(), bins=10, color='salmon', edgecolor='black')\n",
    "plt.title('Distribution of Salary')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## 3.2 Boxplot of Salary by Department\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='Department', y='Salary', data=df)\n",
    "plt.title('Boxplot of Salary by Department')\n",
    "plt.ylabel('Salary')\n",
    "plt.xlabel('Department')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "## 3.3 Scatter plot between Age and Salary\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df['Age'], df['Salary'], alpha=0.6, color='purple')\n",
    "plt.title('Scatter Plot between Age and Salary')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "## 3.4 Heatmap for Correlation\n",
    "plt.figure(figsize=(8, 5))\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Heatmap of Correlation between Numerical Variables')\n",
    "plt.show()\n",
    "\n",
    "## 3.5 Count plot of categorical feature (Department)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Department', data=df, palette='Set2')\n",
    "plt.title('Count Plot of Departments')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
